abstract type AbstractGradientStep{R<:AbstractRBM}
end


abstract type AbstractLoglikelihoodGradientStep{R<:AbstractRBM}
end


mutable struct LoglikelihoodGradientStep{R<:AbstractRBM} <: AbstractLoglikelihoodGradientStep{R}

   gradient::R
   negupdate::Matrix{Float64}
   learningrate::Float64

   function LoglikelihoodGradientStep{R}(rbm::R, learningrate::Float64) where R
      new(deepcopy(rbm), Matrix{Float64}(size(rbm.weights)), learningrate)
   end
end


struct LoglikelihoodGradientStep_GaussianRBM{R <: Union{GaussianBernoulliRBM,
      GaussianBernoulliRBM2}} <: AbstractLoglikelihoodGradientStep{R}

   gradient::R
   negupdate::Matrix{Float64}
   learningrate::Float64
   sdlearningrate::Float64
end


struct BeamAdversarialGradientStep{R<: AbstractRBM} <: AbstractGradientStep{R}
   gradient::R
   negupdate::Matrix{Float64}
   learningrate::Float64
   sdlearningrate::Float64
end


mutable struct BeamGradientStep{R<: AbstractRBM} <: AbstractGradientStep{R}
   loglikelihoodgradientstep::LoglikelihoodGradientStep{R}
   beamadversarialgradientstep::BeamAdversarialGradientStep{R}
   gradient::R
   adversarialweight::Float64
end


function loglikelihoodgradientstep(rbm::AbstractRBM, learningrate::Float64)
   LoglikelihoodGradientStep(rbm, learningrate)
end

function loglikelihoodgradientstep(rbm::GaussianBernoulliRBM2,
      learningrate::Float64;
      sdlearningrate::Float64 = 0.0)

   LoglikelihoodGradientStep_GaussianRBM(deepcopy(rbm),
         Matrix{Float64}(size(rbm.weights)),
         learningrate,
         sdlearningrate)
end


"""
    computegradient!(gradientstep, v, vmodel, h, hmodel, rbm)
Computes the gradient of the RBM `rbm` given the
the hidden activation `h` induced by the sample `v`
and the vectors `vmodel` and `hmodel` generated by sampling from the model.

!!!  note
      This function may alter all arguments except for `rbm` and `hmodel`.
     `hmodel` must not be changed by implementations of `computegradient!`
     since the persistent chain state is stored there.
"""
function computegradient!(
      gradientstep::AbstractLoglikelihoodGradientStep{AbstractRBM},
      v::M, vmodel::M, h::M, hmodel::M, rbm::AbstractRBM
      ) where {M<: AbstractArray{Float64, 2}}

   At_mul_B!(gradient.rbmgradient, v, h)
   At_mul_B!(gradient.negupdate, vmodel, hmodel)
   gradientstep.gradient.weights .-= gradient.negupdate

   gradientstep.gradient.visbias .= vec(mean(v, 1))
   gradientstep.gradient.visbias .-= vec(mean(vmodel, 1))

   gradientstep.gradient.hidbias .= vec(mean(h, 1))
   gradientstep.gradient.hidbias .-= vec(mean(hmodel, 1))
   gradient
end

function computegradient!(
      gradientstep::AbstractLoglikelihoodGradientStep{GaussianBernoulliRBM},
      v::M, vmodel::M, h::M, hmodel::M, gbrbm::GaussianBernoulliRBM
      ) where {M<: AbstractArray{Float64, 2}}

   # See bottom of page 15 in [Krizhevsky, 2009].

   if gradientstep.sdlearningrate > 0.0
      gradientstep.gradient.sd =
            sdupdateterm(gbrbm, v, h) - sdupdateterm(gbrbm, vmodel, hmodel)
   end

   v ./= gbrbm.sd'
   vmodel ./= gbrbm.sd'

   At_mul_B!(gradientstep.gradient.weights, v, h)
   At_mul_B!(gradientstep.negupdate, vmodel, hmodel)
   gradientstep.gradient.weights .-= gradientstep.negupdate

   gradientstep.gradient.hidbias .+= vec(mean(h, 1) - mean(hmodel, 1))
   gradientstep.gradient.visbias .+= vec(mean(v, 1) - mean(vmodel, 1)) ./ gbrbm.sd

   gradient
end

function computegradient!(
      gradientstep::AbstractLoglikelihoodGradientStep{GaussianBernoulliRBM2},
      v::M, vmodel::M, h::M, hmodel::M, gbrbm::GaussianBernoulliRBM2
      ) where {M<: AbstractArray{Float64, 2}}

   # See Cho,
   # "Improved learning of Gaussian-Bernoulli restricted Boltzmann machines"
   sdsq = gbrbm.sd .^ 2

   if gradientstep.sdlearningrate > 0.0
      sdgrads = vmodel .* (hmodel * gbrbm.weights')
      sdgrads .-= v .* (h * gbrbm.weights')
      sdgrads .*= 2.0
      sdgrads .+= (v .- gbrbm.visbias') .^ 2
      sdgrads .-= (vmodel .- gbrbm.visbias') .^ 2
      gradientstep.gradient.sd = vec(mean(sdgrads, 1))
      gradientstep.gradient.sd ./= sdsq
      gradientstep.gradient.sd ./= gbrbm.sd
   end

   v ./= sdsq'
   vmodel ./= sdsq'

   At_mul_B!(gradientstep.gradient.weights, v, h)
   At_mul_B!(gradientstep.negupdate, vmodel, hmodel)
   gradientstep.gradient.weights .-= gradientstep.negupdate

   gradientstep.gradient.hidbias .+= vec(mean(h, 1) - mean(hmodel, 1))
   gradientstep.gradient.visbias .+= vec(mean(v, 1) - mean(vmodel, 1))

   gradient
end

function computegradient!(
      gradientstep::BeamAdversarialGradientStep{AbstractRBM},
      v::M, vmodel::M, h::M, hmodel::M, rbm::AbstractRBM
      ) where {M<: AbstractArray{Float64, 2}}

   critic = nearestneighbourcritic(h, hmodel)

   nvisible = nvisiblenodes(rbm)
   nhidden = nhiddennodes(rbm)

   for i = 1:nvisible
      for j = 1:nhidden
         gradientstep.gradient.weights[i, j] =
               cov(critic, vmodel[:, i] .* hmodel[:, j])
      end
   end

   for i = 1:nvisible
      gradientstep.gradient.visbias[i] = cov(critic, vmodel[:, i])
   end

   for j = 1:nhidden
      gradientstep.gradient.hidbias[j] = cov(critic, hmodel[:, j])
   end

   gradient
end

function computegradient!(
      gradientstep::BeamAdversarialGradientStep{GaussianBernoulliRBM2},
      v::M, vmodel::M, h::M, hmodel::M, rbm::GaussianBernoulliRBM2
      ) where {M<: AbstractArray{Float64, 2}}

   invoke(computegradient!,
         Tuple{BeamAdversarialGradientStep{AbstractRBM}, M, M, M, M, AbstractRBM},
         rbm, v, vmodel, h, hmodel)

   sdsq = rbm.sd .^ 2
   gradientstep.gradient.weights ./= sdsq

   gradient
end

function computegradient!(gradientstep::BeamGradientStep{AbstractRBM},
   v::M, vmodel::M, h::M, hmodel::M, rbm::AbstractRBM
   ) where {M<: AbstractArray{Float64, 2}}

   computegradient!(gradientstep.loglikelihoodgradientstep, v, vmodel, h, hmodel, rbm)
   computegradient!(gradientstep.beamadversarialgradientstep, v, vmodel, h, hmodel, rbm)

   llgrad = gradientstep.loglikelihoodgradientstep.gradient
   advgrad = gradientstep.beamadversarialgradientstep.gradient

   gradientstep.gradient.weights .=
         llgrad.weights * (1 - gradientstep.adversarialweight) +
         advgrad.weights * gradientstep.adversarialweight

   gradientstep.gradient.visbias .=
         llgrad.visbias * (1 - gradientstep.adversarialweight) +
         advgrad.visbias * gradientstep.adversarialweight

   gradientstep.gradient.hidbias .=
         llgrad.hidbias * (1 - gradientstep.adversarialweight) +
         advgrad.hidbias * gradientstep.adversarialweight

   gradient
end

function computegradient!(gradientstep::BeamGradientStep{R},
   v::M, vmodel::M, h::M, hmodel::M, rbm::R
   ) where {M<: AbstractArray{Float64, 2},
         R <:Union{GaussianBernoulliRBM, GaussianBernoulliRBM2}}

   invoke(computegradient!, Tuple{BeamGradientStep{AbstractRBM}, M, M, M, M, })

   llgrad = gradientstep.loglikelihoodgradientstep.gradient
   advgrad = gradientstep.beamadversarialgradientstep.gradient

   gradientstep.gradient.sd .=
      llgrad.sd * (1 - gradientstep.adversarialweight) +
      advgrad.sd * gradientstep.adversarialweight

   gradient
end


"""
    updateparameters!(rbm, gradientstep)
Updates the RBM by walking a step in the direction of the gradient that
has been computed by calling `computegradient!` on `gradientstep`.
"""
function updateparameters!(rbm::R, gradientstep::AbstractGradientStep{R}
      ) where {R <: AbstractRBM}

   gradientstep.gradient.weights .*= gradientstep.learningrate
   gradientstep.gradient.visbias .*= gradientstep.learningrate
   gradientstep.gradient.hidbias .*= gradientstep.learningrate
   rbm.weights .+= gradientstep.gradient.weights
   rbm.visbias .+= gradientstep.gradient.visbias
   rbm.hidbias .+= gradientstep.gradient.hidbias
   rbm
end

function updateparameters!(rbm::Binomial2BernoulliRBM,
      gradientstep::AbstractGradientStep{Binomial2BernoulliRBM})

   # To train a Binomial2BernoulliRBM exactly like
   # training a BernoulliRBM where each two nodes share the weights,
   # use half the learning rate in the visible nodes.
   learningratehidden = gradientstep.learningrate
   learningrate = gradientstep.learningrate / 2.0

   gradientstep.gradient.weights .*= gradientstep.learningrate
   gradientstep.gradient.visbias .*= gradientstep.learningrate
   gradientstep.gradient.hidbias .*= gradientstep.learningratehidden
   rbm.weights .+= gradientstep.gradient.weights
   rbm.visbias .+= gradientstep.gradient.visbias
   rbm.hidbias .+= gradientstep.gradient.hidbias
   rbm
end

function updateparameters!(rbm::R, gradientstep::AbstractGradientStep{R}
      ) where {R<: Union{GaussianBernoulliRBM, GaussianBernoulliRBM2}}

   invoke(updateparameters!, Tuple{AbstractRBM, AbstractGradientStep{AbstractRBM}}, rbm)
   if gradientstep.sdlearningrate > 0.0
      gradientstep.gradient.sd .*= gradientstep.sdlearningrate
      rbm.sd .+= gradientstep.gradient.sd
   end
   rbm
end
