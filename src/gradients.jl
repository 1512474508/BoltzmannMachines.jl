abstract type AbstractGradientStep{R<:AbstractRBM}
end


abstract type AbstractLoglikelihoodGradientStep{R<:AbstractRBM}
end


mutable struct LoglikelihoodGradientStep{R<:AbstractRBM} <: AbstractLoglikelihoodGradientStep{R}

   gradient::R
   negupdate::Matrix{Float64}
   learningrate::Float64

   function LoglikelihoodGradientStep{R}(rbm::R, learningrate::Float64) where R
      new(deepcopy(rbm), Matrix{Float64}(size(rbm.weights)), learningrate)
   end
end


struct LoglikelihoodGradientStep_GaussianRBM{R <: AbstractRBM} <: AbstractLoglikelihoodGradientStep{R}
   gradient::R
   negupdate::Matrix{Float64}
   learningrate::Float64
   sdlearningrate::Float64
end


struct BeamAdversarialGradientStep{R<: AbstractRBM} <: AbstractGradientStep{R}

end


mutable struct BeamGradient{R<: AbstractRBM} <: AbstractGradientStep{R}
   loglikelihoodgradientstep::LoglikelihoodGradientStep
   beamadversarialgradientstep::BeamAdversarialGradientStep
   adversarialweight::Float64
end


function loglikelihoodgradientstep(rbm::AbstractRBM, learningrate::Float64)
   LoglikelihoodGradientStep(rbm, learningrate)
end

function loglikelihoodgradientstep(rbm::GaussianBernoulliRBM2,
      learningrate::Float64;
      sdlearningrate::Float64 = 0.0)

   LoglikelihoodGradientStep_GaussianRBM(deepcopy(rbm),
         Matrix{Float64}(size(rbm.weights)),
         learningrate,
         sdlearningrate)
end


function computegradient!(gradient::AbstractLoglikelihoodGradientStep,
      rbm::AbstractRBM,
      v::M, vmodel::M, h::M, hmodel::M) where {M<: AbstractArray{Float64, 2}}

   At_mul_B!(gradient.rbmgradient, v, h)
   At_mul_B!(gradient.negupdate, vmodel, hmodel)
   gradient.rbmgradient.weights .-= gradient.negupdate
   gradient.rbmgradient.visbias .+= vec(mean(v, 1) - mean(vmodel, 1))
   gradient.rbmgradient.hidbias .+= vec(mean(h, 1) - mean(hmodel, 1))
   gradient
end

function computegradient!(gradient::BeamAdversarialGradientStep, rbm::AbstractRBM,
   v::M, vmodel::M, h::M, hmodel::M) where {M<: AbstractArray{Float64, 2}}

   critic = nearestneighbourcritic(h, hmodel)

   nvisible = nvisiblenodes(rbm)
   nhidden = nhiddennodes(rbm)

   for i = 1:nhidden
      for j = 1:nvisible
         gradient.rbmgradient.weights[i, j] =
               cov(critic, vmodel[:, i] .* hmodel[:, j])
               # TODO bias einbeziehen
      end
   end

   gradient
end

function computegradient!(gradient::BeamAdversarialGradientStep,
      rbm::GaussianBernoulliRBM2,
      v::M, vmodel::M, h::M, hmodel::M) where {M<: AbstractArray{Float64, 2}}

   invoke(computegradient!, Tuple{BeamAdversarialGradientStep, AbstractRBM, M, M, M, M},
         rbm, v, vmodel, h, hmodel)

   sdsq = rbm.sd .^ 2
   gradient.rbmgradient.weights ./= sdsq

   gradient
end


function computegradient!(gradient::BeamGradient, rbm::AbstractRBM,
      v::Matrix{Float64}, vmodel::Matrix{Float64},
      h::Matrix{Float64}, hmodel::Matrix{Float64})

   computegradient!(gradient.loglikelihoodgradient, rbm, v, vmodel, h, hmodel)
   computegradient!(gradient.beamadversarialgradient, rbm, v, vmodel, h, hmodel)
   gradient
end


# TODO change documentation
"""
    updateparameters!(rbm, v, vmodel, h, hmodel, learningrate, sdlearningrate,
            posupdate, negupdate)
Updates the RBM `rbm` given the
the hidden activation `h` induced by the sample
the vectors `vmodel` and `hmodel` generated by Gibbs sampling, the `learningrate`,
the learningrate for the standard deviation `learningratesd` (only relevant for
GaussianBernoulliRBMs) and allocated space for the weights update
as in form of the write-only arguments `posupdate` and `negupdate`.

!!!  note
     `hmodel` must not be changed by implementations of `updateparameters!`
     since the persistent chain state is stored there.
"""
function updateparameters!(gradientstep::AbstractGradientStep{R}, rbm::R
      ) where {R <: AbstractRBM}

   gradientstep.gradient.weights .*= gradientstep.learningrate
   gradientstep.gradient.visbias .*= gradientstep.learningrate
   gradientstep.gradient.hidbias .*= gradientstep.learningrate
   rbm.weights .+= gradientstep.gradient.weights
   rbm.visbias .+= gradientstep.gradient.visbias
   rbm.hidbias .+= gradientstep.gradient.hidbias
   nothing
end


function updateparameters!(gradientstep::AbstractGradientStep{Binomial2BernoulliRBM},
      rbm::Binomial2BernoulliRBM)

   # To train a Binomial2BernoulliRBM exactly like
   # training a BernoulliRBM where each two nodes share the weights,
   # use half the learning rate in the visible nodes.
   learningratehidden = gradientstep.learningrate
   learningrate = gradientstep.learningrate / 2.0

   gradientstep.gradient.weights .*= gradientstep.learningrate
   gradientstep.gradient.visbias .*= gradientstep.learningrate
   gradientstep.gradient.hidbias .*= gradientstep.learningratehidden
   rbm.weights .+= gradientstep.gradient.weights
   rbm.visbias .+= gradientstep.gradient.visbias
   rbm.hidbias .+= gradientstep.gradient.hidbias
   nothing
end

function updateparameters!(gradientstep::AbstractGradientStep{GaussianBernoulliRBM},
   rbm::GaussianBernoulliRBM)

   # See bottom of page 15 in [Krizhevsky, 2009].

   if sdlearningrate > 0.0
      sdgrad = sdupdateterm(gbrbm, v, h) - sdupdateterm(gbrbm, vmodel, hmodel)

      v ./= gbrbm.sd'
      vmodel ./= gbrbm.sd'

      if sdgradclipnorm > 0.0
         sdgradnorm = norm(sdgrad)
         if sdgradnorm > sdgradclipnorm
            rescaling = sdgradclipnorm / sdgradnorm
            sdlearningrate *= rescaling
            learningrate *= rescaling
         end
      end
      sdgrad .*= sdlearningrate
      gbrbm.sd .+= sdgrad
      if any(gbrbm.sd .< 0.0)
         warn("SD-Update leading to negative standard deviation not performed")
         gbrbm.sd .-= sdgrad
      end
   else
      v ./= gbrbm.sd'
      vmodel ./= gbrbm.sd'
   end

   At_mul_B!(posupdate, v, h)
   At_mul_B!(negupdate, vmodel, hmodel)
   posupdate .-= negupdate
   posupdate .*= learningrate
   gbrbm.weights .+= posupdate
   gbrbm.hidbias .+= vec(mean(h, 1) - mean(hmodel, 1)) * learningrate
   gbrbm.visbias .+= vec(mean(v, 1) - mean(vmodel, 1)) ./ gbrbm.sd *
         learningrate
   nothing
end

function updateparameters!(gbrbm::GaussianBernoulliRBM2,
      v::Matrix{Float64}, vmodel::Matrix{Float64},
      h::Matrix{Float64}, hmodel::Matrix{Float64},
      learningrate::Float64,
      sdlearningrate::Float64,
      sdgradclipnorm::Float64,
      posupdate::Matrix{Float64}, negupdate::Matrix{Float64})

   # See Cho,
   # "Improved learning of Gaussian-Bernoulli restricted Boltzmann machines"
   sdsq = gbrbm.sd .^ 2

   if sdlearningrate > 0.0
      sdgrads = vmodel .* (hmodel * gbrbm.weights')
      sdgrads .-= v .* (h * gbrbm.weights')
      sdgrads .*= 2.0
      sdgrads .+= (v .- gbrbm.visbias') .^ 2
      sdgrads .-= (vmodel .- gbrbm.visbias') .^ 2
      sdgrad = vec(mean(sdgrads, 1))
      sdgrad ./= sdsq
      sdgrad ./= gbrbm.sd

      if sdgradclipnorm > 0.0
         sdgradnorm = norm(sdgrad)
         if sdgradnorm > sdgradclipnorm
            rescaling = sdgradclipnorm / sdgradnorm
            sdlearningrate *= rescaling
            learningrate *= rescaling
         end
      end
      sdgrad .*= sdlearningrate
      gbrbm.sd .+= sdgrad
      if any(gbrbm.sd .< 0.0)
         warn("SD-Update leading to negative standard deviation not performed")
         gbrbm.sd .-= sdgrad
      end
   end

   v ./= sdsq'
   vmodel ./= sdsq'

   At_mul_B!(posupdate, v, h)
   At_mul_B!(negupdate, vmodel, hmodel)
   posupdate .-= negupdate
   posupdate .*= learningrate
   gbrbm.weights .+= posupdate
   gbrbm.hidbias .+= vec(mean(h, 1) - mean(hmodel, 1)) * learningrate
   gbrbm.visbias .+= vec(mean(v, 1) - mean(vmodel, 1)) * learningrate
   nothing
end